{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for GitHub\n",
    "\n",
    "# Import Libraries\n",
    "\n",
    "#Import Gradio\n",
    "import gradio as gr\n",
    "\n",
    "#Import helper libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "#Import TensorFlow libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up dataset constraints\n",
    "\n",
    "#Define 6 categories\n",
    "labels = ['plastic','metal','glass','trash','paper', 'cardboard']\n",
    "\n",
    "path = r'C:/Users/Sammy/garbage/'\n",
    "\n",
    "#Image parameters\n",
    "WIDTH = 224\n",
    "HEIGHT = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1773 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255)\n",
    "\n",
    "train= train_datagen.flow_from_directory(\n",
    "        path + 'Train/',\n",
    "        target_size=(WIDTH, HEIGHT),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 252 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "val_datagen = ImageDataGenerator(\n",
    "        rescale=1./255)\n",
    "\n",
    "val = val_datagen.flow_from_directory(\n",
    "        path + 'Validation/',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Import VGG16 ImageNet weights for initialisation\n",
    "\n",
    "conv_base = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(WIDTH, HEIGHT, 3))\n",
    "\n",
    "conv_base.trainable = False\n",
    "\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               6422784   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 6)                 1542      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,139,014\n",
      "Trainable params: 6,424,326\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.7))\n",
    "model.add(layers.Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "56/56 [==============================] - 71s 1s/step - loss: 2.1716 - accuracy: 0.2916 - val_loss: 1.3906 - val_accuracy: 0.5079\n",
      "Epoch 2/60\n",
      "56/56 [==============================] - 70s 1s/step - loss: 1.5000 - accuracy: 0.3920 - val_loss: 1.4238 - val_accuracy: 0.3611\n",
      "Epoch 3/60\n",
      "56/56 [==============================] - 70s 1s/step - loss: 1.3910 - accuracy: 0.4174 - val_loss: 1.3145 - val_accuracy: 0.5913\n",
      "Epoch 4/60\n",
      "56/56 [==============================] - 69s 1s/step - loss: 1.3401 - accuracy: 0.4411 - val_loss: 1.3169 - val_accuracy: 0.4603\n",
      "Epoch 5/60\n",
      "56/56 [==============================] - 69s 1s/step - loss: 1.2958 - accuracy: 0.4597 - val_loss: 1.2997 - val_accuracy: 0.5833\n",
      "Epoch 6/60\n",
      "56/56 [==============================] - 70s 1s/step - loss: 1.2156 - accuracy: 0.4743 - val_loss: 1.2008 - val_accuracy: 0.5595\n",
      "Epoch 7/60\n",
      "56/56 [==============================] - 69s 1s/step - loss: 1.2081 - accuracy: 0.4800 - val_loss: 1.2709 - val_accuracy: 0.5794\n",
      "Epoch 8/60\n",
      "56/56 [==============================] - 69s 1s/step - loss: 1.1586 - accuracy: 0.4890 - val_loss: 1.2827 - val_accuracy: 0.5635\n",
      "Epoch 9/60\n",
      "56/56 [==============================] - 70s 1s/step - loss: 1.1381 - accuracy: 0.5082 - val_loss: 1.3434 - val_accuracy: 0.5794\n",
      "Epoch 10/60\n",
      "56/56 [==============================] - 71s 1s/step - loss: 1.0805 - accuracy: 0.5324 - val_loss: 1.2484 - val_accuracy: 0.5913\n",
      "Epoch 11/60\n",
      "56/56 [==============================] - 70s 1s/step - loss: 1.1464 - accuracy: 0.5059 - val_loss: 1.2612 - val_accuracy: 0.5794\n",
      "Epoch 12/60\n",
      "56/56 [==============================] - 71s 1s/step - loss: 1.1254 - accuracy: 0.5285 - val_loss: 1.2382 - val_accuracy: 0.6190\n",
      "Epoch 13/60\n",
      "56/56 [==============================] - 70s 1s/step - loss: 1.0929 - accuracy: 0.5257 - val_loss: 1.3468 - val_accuracy: 0.5675\n",
      "Epoch 14/60\n",
      "56/56 [==============================] - 70s 1s/step - loss: 1.0479 - accuracy: 0.5285 - val_loss: 1.3520 - val_accuracy: 0.5675\n",
      "Epoch 15/60\n",
      "56/56 [==============================] - 68s 1s/step - loss: 1.0333 - accuracy: 0.5257 - val_loss: 1.4003 - val_accuracy: 0.5516\n",
      "Epoch 16/60\n",
      "56/56 [==============================] - 68s 1s/step - loss: 0.9893 - accuracy: 0.5702 - val_loss: 1.2147 - val_accuracy: 0.6270\n",
      "Epoch 17/60\n",
      "56/56 [==============================] - 68s 1s/step - loss: 0.9930 - accuracy: 0.5747 - val_loss: 1.3381 - val_accuracy: 0.6151\n",
      "Epoch 18/60\n",
      "56/56 [==============================] - 68s 1s/step - loss: 0.9826 - accuracy: 0.5770 - val_loss: 1.3812 - val_accuracy: 0.5595\n",
      "Epoch 19/60\n",
      "56/56 [==============================] - 68s 1s/step - loss: 0.9244 - accuracy: 0.5950 - val_loss: 1.2609 - val_accuracy: 0.6151\n",
      "Epoch 20/60\n",
      "56/56 [==============================] - 69s 1s/step - loss: 0.9632 - accuracy: 0.5702 - val_loss: 1.4268 - val_accuracy: 0.6230\n",
      "Epoch 21/60\n",
      "56/56 [==============================] - 70s 1s/step - loss: 1.0061 - accuracy: 0.5544 - val_loss: 1.3398 - val_accuracy: 0.6429\n",
      "Epoch 22/60\n",
      "56/56 [==============================] - 68s 1s/step - loss: 0.9173 - accuracy: 0.5900 - val_loss: 1.4406 - val_accuracy: 0.6429\n",
      "Epoch 23/60\n",
      "56/56 [==============================] - 68s 1s/step - loss: 0.8939 - accuracy: 0.6176 - val_loss: 1.3502 - val_accuracy: 0.6230\n",
      "Epoch 24/60\n",
      "56/56 [==============================] - 68s 1s/step - loss: 0.8536 - accuracy: 0.6373 - val_loss: 1.3491 - val_accuracy: 0.6270\n",
      "Epoch 25/60\n",
      "56/56 [==============================] - 68s 1s/step - loss: 0.8823 - accuracy: 0.6221 - val_loss: 1.2565 - val_accuracy: 0.6111\n",
      "Epoch 26/60\n",
      "56/56 [==============================] - 68s 1s/step - loss: 0.9154 - accuracy: 0.6024 - val_loss: 1.4795 - val_accuracy: 0.5675\n",
      "Epoch 27/60\n",
      "56/56 [==============================] - 68s 1s/step - loss: 0.8872 - accuracy: 0.6125 - val_loss: 1.4775 - val_accuracy: 0.5992\n",
      "Epoch 28/60\n",
      "56/56 [==============================] - 68s 1s/step - loss: 0.8804 - accuracy: 0.6227 - val_loss: 1.3898 - val_accuracy: 0.6190\n",
      "Epoch 29/60\n",
      "56/56 [==============================] - 68s 1s/step - loss: 0.8729 - accuracy: 0.6182 - val_loss: 1.6413 - val_accuracy: 0.6032\n",
      "Epoch 30/60\n",
      "56/56 [==============================] - 68s 1s/step - loss: 0.8896 - accuracy: 0.6103 - val_loss: 1.5644 - val_accuracy: 0.5913\n",
      "Epoch 31/60\n",
      "56/56 [==============================] - 68s 1s/step - loss: 0.8662 - accuracy: 0.6097 - val_loss: 1.4201 - val_accuracy: 0.5913\n",
      "Epoch 32/60\n",
      "56/56 [==============================] - 70s 1s/step - loss: 0.8529 - accuracy: 0.6306 - val_loss: 1.2903 - val_accuracy: 0.6310\n",
      "Epoch 33/60\n",
      "56/56 [==============================] - 70s 1s/step - loss: 0.8081 - accuracy: 0.6418 - val_loss: 1.6034 - val_accuracy: 0.6548\n",
      "Epoch 34/60\n",
      "56/56 [==============================] - 70s 1s/step - loss: 0.8092 - accuracy: 0.6362 - val_loss: 1.3667 - val_accuracy: 0.6468\n",
      "Epoch 35/60\n",
      "56/56 [==============================] - 70s 1s/step - loss: 0.7936 - accuracy: 0.6610 - val_loss: 1.6283 - val_accuracy: 0.6587\n",
      "Epoch 36/60\n",
      "56/56 [==============================] - 70s 1s/step - loss: 0.7582 - accuracy: 0.6678 - val_loss: 1.4793 - val_accuracy: 0.6508\n",
      "Epoch 37/60\n",
      "56/56 [==============================] - 70s 1s/step - loss: 0.7965 - accuracy: 0.6554 - val_loss: 1.3283 - val_accuracy: 0.6429\n",
      "Epoch 38/60\n",
      "56/56 [==============================] - 69s 1s/step - loss: 0.8068 - accuracy: 0.6469 - val_loss: 1.5219 - val_accuracy: 0.6508\n",
      "Epoch 39/60\n",
      "56/56 [==============================] - 70s 1s/step - loss: 0.7634 - accuracy: 0.6729 - val_loss: 1.4500 - val_accuracy: 0.6389\n",
      "Epoch 40/60\n",
      "56/56 [==============================] - 70s 1s/step - loss: 0.7190 - accuracy: 0.6949 - val_loss: 1.3519 - val_accuracy: 0.6310\n",
      "Epoch 41/60\n",
      "56/56 [==============================] - 71s 1s/step - loss: 0.7740 - accuracy: 0.6768 - val_loss: 1.3499 - val_accuracy: 0.6429\n",
      "Epoch 42/60\n",
      "56/56 [==============================] - 68s 1s/step - loss: 0.7612 - accuracy: 0.6701 - val_loss: 1.6185 - val_accuracy: 0.6429\n",
      "Epoch 43/60\n",
      "56/56 [==============================] - 68s 1s/step - loss: 0.7664 - accuracy: 0.6779 - val_loss: 1.6015 - val_accuracy: 0.6468\n",
      "Epoch 44/60\n",
      "56/56 [==============================] - 70s 1s/step - loss: 0.7397 - accuracy: 0.6763 - val_loss: 1.7213 - val_accuracy: 0.6389\n",
      "Epoch 45/60\n",
      "56/56 [==============================] - 70s 1s/step - loss: 0.7361 - accuracy: 0.6695 - val_loss: 1.5423 - val_accuracy: 0.6270\n",
      "Epoch 46/60\n",
      "56/56 [==============================] - 70s 1s/step - loss: 0.7559 - accuracy: 0.6661 - val_loss: 1.6528 - val_accuracy: 0.6349\n",
      "Epoch 47/60\n",
      "56/56 [==============================] - 70s 1s/step - loss: 0.7235 - accuracy: 0.6746 - val_loss: 1.6296 - val_accuracy: 0.6627\n",
      "Epoch 48/60\n",
      "56/56 [==============================] - 70s 1s/step - loss: 0.7274 - accuracy: 0.6898 - val_loss: 1.5161 - val_accuracy: 0.6310\n",
      "Epoch 49/60\n",
      "56/56 [==============================] - 71s 1s/step - loss: 0.7334 - accuracy: 0.6785 - val_loss: 1.4774 - val_accuracy: 0.5952\n",
      "Epoch 50/60\n",
      "56/56 [==============================] - 71s 1s/step - loss: 0.7667 - accuracy: 0.6667 - val_loss: 1.7637 - val_accuracy: 0.6310\n",
      "Epoch 51/60\n",
      "56/56 [==============================] - 75s 1s/step - loss: 0.7276 - accuracy: 0.6870 - val_loss: 1.8244 - val_accuracy: 0.6349\n",
      "Epoch 52/60\n",
      "56/56 [==============================] - 73s 1s/step - loss: 0.6858 - accuracy: 0.6983 - val_loss: 1.4557 - val_accuracy: 0.6587\n",
      "Epoch 53/60\n",
      "56/56 [==============================] - 68s 1s/step - loss: 0.7149 - accuracy: 0.6926 - val_loss: 1.8029 - val_accuracy: 0.6111\n",
      "Epoch 54/60\n",
      "56/56 [==============================] - 68s 1s/step - loss: 0.6561 - accuracy: 0.7225 - val_loss: 1.7839 - val_accuracy: 0.6310\n",
      "Epoch 55/60\n",
      "56/56 [==============================] - 68s 1s/step - loss: 0.7507 - accuracy: 0.6757 - val_loss: 1.4679 - val_accuracy: 0.6587\n",
      "Epoch 56/60\n",
      "56/56 [==============================] - 68s 1s/step - loss: 0.6821 - accuracy: 0.7118 - val_loss: 1.5201 - val_accuracy: 0.6389\n",
      "Epoch 57/60\n",
      "56/56 [==============================] - 68s 1s/step - loss: 0.7110 - accuracy: 0.6971 - val_loss: 1.7502 - val_accuracy: 0.6468\n",
      "Epoch 58/60\n",
      "56/56 [==============================] - 68s 1s/step - loss: 0.6949 - accuracy: 0.7033 - val_loss: 1.7017 - val_accuracy: 0.6349\n",
      "Epoch 59/60\n",
      "56/56 [==============================] - 68s 1s/step - loss: 0.6681 - accuracy: 0.7050 - val_loss: 1.8574 - val_accuracy: 0.6548\n",
      "Epoch 60/60\n",
      "56/56 [==============================] - 68s 1s/step - loss: 0.6925 - accuracy: 0.6909 - val_loss: 1.9110 - val_accuracy: 0.6349\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 60\n",
    "\n",
    "history = model.fit(train, batch_size=batch_size, epochs=epochs,validation_data=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gradio Image Prediction Function\n",
    "\n",
    "def predict(img):\n",
    "    img = img.reshape(-1, WIDTH, HEIGHT, 3)\n",
    "    pred = model.predict(img).tolist()[0]\n",
    "    return {labels[i]: pred[i] for i in range(6)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gradio Constraints\n",
    "image = gr.inputs.Image(shape=(WIDTH, HEIGHT))\n",
    "label = gr.outputs.Label(num_top_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some example images for Gradio Interface\n",
    "\n",
    "filenames_test = [path + 'Examples/paper/paper1.jpg', path + 'Examples/plastic/plastic1.jpg',\n",
    "                 path + 'Examples/glass/glass1.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add background image\n",
    "\n",
    "css_code=\"body {background-image: url('file=C:/Users/Sammy/garbage/background.jpg')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://29896.gradio.app\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting, check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://29896.gradio.app\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x24ffa87f610>,\n",
       " 'http://127.0.0.1:7860/',\n",
       " 'https://29896.gradio.app')"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Launch Gradio\n",
    "\n",
    "gr.Interface(fn=predict, inputs=image, outputs=label, interpretation='default',\n",
    "             examples = filenames_test, css=css_code).launch(debug='True', share=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "twFQbltnm8da",
    "cfTL_5kPsl4j"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3516ae58402408c1c8fee4af5eaa5191c31b327d45446ded1cca9258c0051614"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
